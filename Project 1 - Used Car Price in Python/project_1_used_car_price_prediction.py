# -*- coding: utf-8 -*-
"""Project 1 - Used Car price Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CfK1C46Q1K-WHo5q2hFxEmUi2rWz5qwz

# **Import the Depedencies**
*   pandas - making dataframes
*   matplotlib& seaborn - used to make graphs & plots
*   sklearn - to perform regression / classification
*   metrics - to identify outliers / errors etc.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

"""# **Data Collection and preprocessing**

"""

# making dataframes from the csv

car_dataset = pd.read_csv('/content/used_car_dataset.csv')

# 5 rows of the data are shown
car_dataset.head()

# checking the number of rows and columns
car_dataset.shape

#checking info about the dataset
car_dataset.info()

"""# **Handling Missing values**"""

car_dataset.isnull().sum()

"""# **Handling Duplicated Values**"""

Duplicate_Value = car_dataset.duplicated().sum()
print(f'Number of duplicated rows are = {Duplicate_Value}')

car_dataset = car_dataset.drop_duplicates()

print("Number of Duplicate values are =" ,car_dataset.duplicated().sum())

"""# **Distribution of Categorical data**"""

print(car_dataset.fuel_type.value_counts())
print(car_dataset.city.value_counts())

car_dataset.info()

import re

# this function regular expression library to convert string into integers
# `"abc123.456def"` -> `123.456`
def preprocess_string(s):
    s = re.sub(r'[^\d.]', '',s)

    return float(s)

# Apply preprocessing to convert price to float

car_dataset['car_price_in_rupees'] = car_dataset['car_price_in_rupees'].apply(preprocess_string)

# Apply preprocessing to convert kms_driven to int

car_dataset['kms_driven'] = car_dataset['kms_driven'].apply(preprocess_string).astype(int)

car_dataset.info()

"""# **Handling Outliers**"""

# Apply visualization to understand the data

plt.boxplot(car_dataset['kms_driven'])
plt.title('Box Plot of kms_driven')
plt.show()

"""# **Identifing Outliers using IQR**"""

# the quantile() method is often used with pandas DataFrames or Series objects to calculate specific quantiles of a dataset.
Q1 = car_dataset['kms_driven'].quantile(0.25)
Q3 = car_dataset['kms_driven'].quantile(0.75)
IQR = Q3 - Q1

# Using IQR to Filter Outliers
car_dataset_filtered = car_dataset[(car_dataset['kms_driven'] >= Q1 - 1.5 * IQR) & (car_dataset['kms_driven'] <= Q3 + 1.5 * IQR)]

car_dataset = car_dataset_filtered

plt.boxplot(car_dataset['kms_driven'])
plt.title('Box Plot of kms_driven (After Removing Outliers)')
plt.show()

car_dataset.shape

car_dataset.head()

"""# **Data Cleaning for fuel_type**"""

car_dataset.fuel_type.unique()

# Visualizing the fuel type using plots

sns.histplot(car_dataset['fuel_type'])

car_dataset.fuel_type.unique()

# Replace any occurrence of "Diesel + 1" in the "fuel_type" column with just "Diesel".
car_dataset.loc[car_dataset["fuel_type"] == "Diesel + 1", "fuel_type"] = "Diesel"

# Replace any occurrence of "Petrol + 1" in the "fuel_type" column with just "Petrol".
car_dataset.loc[car_dataset["fuel_type"] == "Petrol + 1", "fuel_type"] = "Petrol"

car_dataset.fuel_type.unique()

sns.histplot(car_dataset['fuel_type'])

"""## **Removing LPG and Hybrid as they are low in numbers**"""

car_dataset = car_dataset.drop(car_dataset[(car_dataset.fuel_type == "Hybrid")].index)
car_dataset = car_dataset.drop(car_dataset[(car_dataset.fuel_type == "LPG")].index)

car_dataset.fuel_type.unique()

sns.histplot(car_dataset['fuel_type'])

"""## **Combine Electric and CNG to Eco Fuel**"""

car_dataset['fuel_type'] = car_dataset['fuel_type'].replace(['Electric', 'CNG'], 'alternative_eco_fuel')

car_dataset.fuel_type.unique()

sns.histplot(car_dataset['fuel_type'])

"""# **One Hot Encoding**"""

# Creating binary indicator columns for each fuel type present in the dataset.
car_dataset['fuel_Petrol'] = car_dataset.apply(lambda row: 1 if (row["fuel_type"] == "Petrol") else 0, axis=1)
car_dataset['fuel_Diesel'] = car_dataset.apply(lambda row: 1 if (row["fuel_type"] == "Diesel") else 0, axis=1)
car_dataset['fuel_alternative_eco'] = car_dataset.apply(lambda row: 1 if (row["fuel_type"] == "alternative_eco_fuel") else 0, axis=1)

# dropping fuel_type and using binary indicator, we are representing the fuel

car_dataset.drop(columns=['fuel_type'], inplace=True)

car_dataset.head()

plt.subplots(figsize=(12, 4))
plt.subplot(1, 3, 1)
sns.boxplot(car_dataset['fuel_Petrol'])

plt.subplot(1, 3, 2)
sns.boxplot(car_dataset['fuel_Petrol'])

plt.subplot(1, 3, 3)
sns.boxplot(car_dataset['fuel_alternative_eco'])

# IQR - Interquartile Range

Q1 = car_dataset['fuel_alternative_eco'].quantile(0.25)
Q3 = car_dataset['fuel_alternative_eco'].quantile(0.75)

IQR = Q3 - Q1
# Using IQR to Filter Outliers
car_dataset_filtered = car_dataset[(car_dataset['fuel_alternative_eco'] >= Q1 - 1.5 * IQR) & (car_dataset['fuel_alternative_eco'] <= Q3 + 1.5 * IQR)]

car_dataset = car_dataset_filtered

plt.boxplot(car_dataset['fuel_alternative_eco'])
plt.title('Box Plot of fuel_alternative_eco (After Removing Outliers)')
plt.show()

"""## **Data Cleaning for Year of Manufacture**"""

# Converting years to numbers by 2024 - year
car_dataset['age'] = car_dataset.apply(lambda row: 2024 - row["year_of_manufacture"], axis=1)

car_dataset.drop(columns=['year_of_manufacture'], inplace=True)

car_dataset.head()

"""## **Handling Outliers**"""

plt.boxplot(car_dataset['age'])
plt.title('Box Plot of age')
plt.show()

# IQR - Interquartile Range

Q1 = car_dataset['age'].quantile(0.25)
Q3 = car_dataset['age'].quantile(0.75)

IQR = Q3 - Q1
# Using IQR to Filter Outliers
car_dataset_filtered = car_dataset[(car_dataset['age'] >= Q1 - 1.5 * IQR) & (car_dataset['age'] <= Q3 + 1.5 * IQR)]

car_dataset = car_dataset_filtered

plt.boxplot(car_dataset['age'])
plt.title('Box Plot of age')
plt.show()

"""## **Data Cleaning for Car Name**"""

# By creating a new column called "car brand", we will clean the data

car_dataset.insert(1, "car_brand", car_dataset["car_name"].str.split(" ").str[0])
car_dataset.car_brand.unique()

car_brands = ['Mercedes-Benz', 'BMW', 'Audi', 'Toyota', 'Volkswagen', 'Porsche', 'Volvo', 'Jaguar', 'Land',
              'Ford', 'Honda', 'Chevrolet', 'Hyundai', 'Kia', 'Nissan', 'Renault', 'Skoda', 'MINI', 'MG', 'Mahindra',
              'Tata', 'Isuzu', 'Jeep', 'Datsun', 'Fiat', 'Citroen', 'Bentley','Maruti']

for brand_name in car_brands:
    car_dataset.loc[car_dataset["car_brand"] == brand_name, "car_brand"] = ((car_brands.index(brand_name) + 1)
     / len(car_dataset)) * 100

car_dataset.car_brand.unique()

car_dataset.rename(columns={'car_brand': 'car_brand_score'}, inplace=True)

car_dataset.drop(columns=['car_name'], inplace=True)
car_dataset.head()

"""## **Data Cleaning for City**"""

# as city is irrelevant here for predicting used car price
# lets drop the city column
car_dataset.drop(columns=['city'], inplace=True)
car_dataset.head()

"""## **Independant and Dependant features**"""

# y contains the dependant variable and x contains the independant
# we need to predict the dependant feature with the given indepedant features
X=car_dataset.drop('car_price_in_rupees',axis=1)
Y=car_dataset['car_price_in_rupees']

X.head()

Y.head()

# train_test_split function splits arrays or matrices into random train and test subsets.
# test_size = 30% will be used for testing and random_state ensures same data is produced
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=42)

X_train.shape, X_test.shape

Y_train.shape, Y_test.shape

"""## **Checking for Multicollinearity**"""

# multicollinearity is a situation in which two or more independent variables
# in a regression model are highly correlated with each other.
plt.figure(figsize=(10,8))
corr=X_train.corr()
sns.heatmap(corr,annot=True)

"""## **Feature Scaling Or Standardization**"""

# The StandardScaler class is used for standardizing features
# by removing the mean and scaling to unit variance.
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

# fit_transform is used to fit the training data to the scalar instance
# this allows the both test and training data to be consistent
X_train_scaled=scaler.fit_transform(X_train)
X_test_scaled=scaler.transform(X_test)

X_train_scaled

plt.subplots(figsize=(25, 6))
plt.subplot(1, 2, 1)
sns.boxplot(data=X_train)
plt.title('X_train Before Scaling')
plt.subplot(1, 2, 2)
sns.boxplot(data=X_train_scaled)
plt.title('X_train After Scaling')

"""# **Model Seclection**

### **Linear Regression**
"""

linreg=LinearRegression()

# Training the model on the training set
""" The fit() method trains the model by adjusting
 its parameters to minimize the difference between
 the predicted values and the actual target values. """
linreg.fit(X_train_scaled,Y_train)

# Making predictions on the testing set
""""The predict() method generates predicted
    target values (Y_pred) based on the input features."""
Y_pred=linreg.predict(X_test_scaled)

# Evaluating the model
"""MAE measures the average absolute difference
   between the predicted values and the actual values."""
"""R2 score calculated by comparing the variance of the
    predicted values to the variance of the actual values."""
mae=mean_absolute_error(Y_test,Y_pred)
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

"""## **Lasso Regression**"""

from sklearn.linear_model import Lasso

# Initializing the Lasso
lasso=Lasso()

# Training the model on the training set
lasso.fit(X_train_scaled,Y_train)
Y_pred=lasso.predict(X_test_scaled)

# Making predictions on the testing set
mae=mean_absolute_error(Y_test,Y_pred)

# Evaluating the model
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

"""### **KNN Regression**"""

from sklearn.neighbors import KNeighborsRegressor

# Initializing the KNN regressor
knn_regressor = KNeighborsRegressor(n_neighbors=3)  # adjusting the number of neighbors to 3

# Training the model on the training set
knn_regressor.fit(X_train_scaled, Y_train)

# Making predictions on the testing set
Y_pred = knn_regressor.predict(X_test_scaled)

# Evaluating the model
mae=mean_absolute_error(Y_test,Y_pred)
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

"""### **AdaBoost**"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initializing a base regressor (e.g., Decision Tree Regressor)
base_regressor = DecisionTreeRegressor(max_depth=1) # adjusting Weak learner, as needed

# Initializing the AdaBoost regressor
adaboost_regressor = AdaBoostRegressor(base_regressor, n_estimators=50, random_state=42)

# Train the model on the training set
adaboost_regressor.fit(X_train_scaled, Y_train)

# Make predictions on the testing set
Y_pred = adaboost_regressor.predict(X_test_scaled)

# Evaluating the model
mae=mean_absolute_error(Y_test,Y_pred)
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

"""### **Random Forest Regression**"""

from sklearn.ensemble import RandomForestRegressor

# Initializing the Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the model on the training set
rf.fit(X_train_scaled, Y_train)

# Making predictions on the testing set
Y_pred = rf.predict(X_test_scaled)

# Evaluating the model
mae=mean_absolute_error(Y_test,Y_pred)
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

"""Model selection is the act of selecting perfect model which gives
minimum MAE and maximum RAE, here random forest regression satisfies it
so lets select that model for prediction

# **Creating ML Model using Random Forest Regressor**
"""

import pickle # To save model for future use

from sklearn.ensemble import RandomForestRegressor

# Initializing the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the model on the training set
model.fit(X_train_scaled, Y_train)

# Making predictions on the testing set
Y_pred = model.predict(X_test_scaled)

# Evaluating the model
mae=mean_absolute_error(Y_test,Y_pred)
score=r2_score(Y_test,Y_pred)

print("Mean absolute error", mae)
print("R2 Score", score)

plt.scatter(Y_test,Y_pred)

# Writes the data into a file called car.price.pk1 and saves it for later use
pickle.dump(model , open('car_price.pk1' , 'wb'))

"""# **Final Predictions**"""

"""This line imports the pickle module, which
    provides functions for serializing and deserializing Python objects."""
import pickle

car_price_model = pickle.load(open('car_price.pk1' , 'rb'))

car_price_model_accuracy = car_price_model.score(X_test_scaled, Y_test)

print("Car Price Model Accuracy:" , car_price_model_accuracy * 100 , "%")

